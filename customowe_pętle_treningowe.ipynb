{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoki 1\n",
      "Epoka 1 Krok 0: Strata = 2.317410707473755 Dokładność = 0.21875\n",
      "Epoka 1 Krok 200: Strata = 0.42393308877944946 Dokładność = 0.8376865386962891\n",
      "Epoka 1 Krok 400: Strata = 0.22326050698757172 Dokładność = 0.8682979941368103\n",
      "Epoka 1 Krok 600: Strata = 0.1828051209449768 Dokładność = 0.8830074667930603\n",
      "Epoka 1 Krok 800: Strata = 0.15207862854003906 Dokładność = 0.8954041600227356\n",
      "Epoka 1 Krok 1000: Strata = 0.4218965470790863 Dokładność = 0.9026286005973816\n",
      "Epoka 1 Krok 1200: Strata = 0.21066373586654663 Dokładność = 0.9094504714012146\n",
      "Epoka 1 Krok 1400: Strata = 0.2375975102186203 Dokładność = 0.9144583940505981\n",
      "Epoka 1 Krok 1600: Strata = 0.2724875807762146 Dokładność = 0.9179614186286926\n",
      "Epoka 1 Krok 1800: Strata = 0.2214605212211609 Dokładność = 0.9220398664474487\n",
      "Koniec epoki 1, strata: 0.0289167333394289, dokładność: 0.923966646194458\n",
      "Start epoki 2\n",
      "Epoka 2 Krok 0: Strata = 0.08052361011505127 Dokładność = 1.0\n",
      "Epoka 2 Krok 200: Strata = 0.1263689547777176 Dokładność = 0.9617537260055542\n",
      "Epoka 2 Krok 400: Strata = 0.1255946159362793 Dokładność = 0.9589307904243469\n",
      "Epoka 2 Krok 600: Strata = 0.04775157570838928 Dokładność = 0.9614184498786926\n",
      "Epoka 2 Krok 800: Strata = 0.0832323282957077 Dokładność = 0.9624297618865967\n",
      "Epoka 2 Krok 1000: Strata = 0.2837260067462921 Dokładność = 0.9629433155059814\n",
      "Epoka 2 Krok 1200: Strata = 0.13018937408924103 Dokładność = 0.9635720252990723\n",
      "Epoka 2 Krok 1400: Strata = 0.16923092305660248 Dokładność = 0.9641104340553284\n",
      "Epoka 2 Krok 1600: Strata = 0.21103999018669128 Dokładność = 0.9642020463943481\n",
      "Epoka 2 Krok 1800: Strata = 0.1426324099302292 Dokładność = 0.9649326801300049\n",
      "Koniec epoki 2, strata: 0.04251421242952347, dokładność: 0.9655500054359436\n"
     ]
    }
   ],
   "source": [
    "# Niestandardowe pętle treningowe z własno zdefiniowanym Callbackiem\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# --- Krok 1: Przygotowanie danych ---\n",
    "# Wczytanie zbioru danych MNIST (obrazy cyfr 28x28 pikseli z etykietami od 0 do 9)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacja wartości pikseli – przeskalowanie do zakresu 0–1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Tworzenie zbioru danych treningowych z podziałem na partie (batch size = 32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# --- Krok 2: Definicja modelu ---\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),         # Spłaszczenie obrazu do wektora długości 784\n",
    "    Dense(128, activation='relu'),         # Warstwa ukryta z 128 neuronami i aktywacją ReLU\n",
    "    Dense(10)                               # Warstwa wyjściowa – po jednej wartości na każdą z 10 klas\n",
    "])\n",
    "\n",
    "# --- Krok 3: Funkcja straty, optymalizator, metryka ---\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Funkcja straty dla klasyfikacji wieloklasowej\n",
    "optimizer = tf.keras.optimizers.Adam()  # Optymalizator Adam – skuteczny i popularny\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metryka dokładności\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# --- Krok 4: Własna callback-klasa ---\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Wypisuje wynik na końcu każdej epoki\n",
    "        print(f'Koniec epoki {epoch + 1}, strata: {logs.get(\"loss\")}, dokładność: {logs.get(\"accuracy\")}')\n",
    "\n",
    "\n",
    "# --- Krok 5: Niestandardowa pętla treningowa z użyciem własnego callbacka ---\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Inicjalizacja własnego callbacka\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start epoki {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Przejście w przód – model tworzy przewidywania\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Obliczenie wartości straty\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Obliczenie gradientów względem wag\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Aktualizacja wag modelu\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Aktualizacja metryki dokładności\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Co 200 kroków wypisujemy aktualny stan\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoka {epoch + 1} Krok {step}: Strata = {loss_value.numpy()} Dokładność = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Na koniec epoki wywołujemy nasz własny callback\n",
    "    custom_callback.on_epoch_end(epoch, logs={\n",
    "        'loss': loss_value.numpy(),\n",
    "        'accuracy': accuracy_metric.result().numpy()\n",
    "    })\n",
    "\n",
    "    # Resetowanie metryki przed kolejną epoką\n",
    "    accuracy_metric.reset_state()  # Uwaga: reset_state() (nie reset_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niestandardowe pętle treningowe z dodaną metryką dokładności:\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# --- Krok 1: Przygotowanie danych ---\n",
    "# Wczytanie zbioru danych MNIST (obrazy 28x28 pikseli przedstawiające cyfry 0-9)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacja wartości pikseli (z zakresu 0-255 na zakres 0-1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Utworzenie zbioru treningowego z podziałem na partie (batch size = 32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# --- Krok 2: Definicja modelu ---\n",
    "# Model sekwencyjny: wejście → warstwa ukryta → wyjście\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),          # Spłaszczenie obrazu 28x28 pikseli do wektora 784 elementów\n",
    "    Dense(128, activation='relu'),          # Warstwa ukryta z 128 neuronami i aktywacją ReLU\n",
    "    Dense(10)                                # Warstwa wyjściowa z 10 neuronami (po jednym na każdą klasę 0-9)\n",
    "])\n",
    "\n",
    "# --- Krok 3: Definicja funkcji straty, optymalizatora i metryki ---\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Funkcja straty dla klasyfikacji wieloklasowej\n",
    "optimizer = tf.keras.optimizers.Adam()  # Optymalizator Adam – szybka i skuteczna metoda aktualizacji wag\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metryka dokładności – ile etykiet zostało poprawnie przewidzianych\n",
    "\n",
    "\n",
    "# --- Krok 4: Niestandardowa pętla treningowa z mierzoną dokładnością ---\n",
    "epochs = 5  # Liczba epok (pełnych przejść przez dane treningowe)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start epoki {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Przejście w przód – model generuje przewidywania (logity)\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Obliczenie straty porównując przewidywania z prawdziwymi etykietami\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Obliczenie gradientów względem wag modelu\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Zastosowanie gradientów – aktualizacja wag w modelu\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Zaktualizowanie metryki dokładności (dla aktualnej partii danych)\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Wypisanie straty i dokładności co 200 kroków\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoka {epoch + 1} Krok {step}: Strata = {loss_value.numpy()} Dokładność = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset metryki po zakończeniu każdej epoki, by zacząć mierzyć od zera\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niestandardowe pętle treningowe:\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import numpy as np\n",
    "\n",
    "# Wyłączenie ostrzeżeń Pythona\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ustawienie poziomu logowania TensorFlow (0 = wszystkie logi, 2 = tylko błędy)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# --- Krok 1: Przygotowanie danych ---\n",
    "# Załadowanie zestawu danych MNIST (obrazy cyfr 28x28 pikseli)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacja danych - przeskalowanie wartości pikseli z zakresu 0-255 do 0-1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Tworzenie obiektu zbioru treningowego z podziałem na partie (batch size 32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# --- Krok 2: Zdefiniowanie modelu ---\n",
    "# Model sekwencyjny z warstwami: spłaszczającą, ukrytą i wyjściową\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),         # Spłaszczenie obrazu 28x28 do wektora 784\n",
    "    Dense(128, activation='relu'),         # Warstwa ukryta z 128 neuronami i aktywacją ReLU\n",
    "    Dense(10)                               # Warstwa wyjściowa z 10 neuronami (po 1 dla każdej cyfry 0-9)\n",
    "])\n",
    "\n",
    "# --- Krok 3: Funkcja straty i optymalizator ---\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Funkcja straty dla klasyfikacji wieloklasowej\n",
    "optimizer = tf.keras.optimizers.Adam()  # Optymalizator Adam\n",
    "\n",
    "# --- Krok 4: Ręczna pętla treningowa (custom training loop) ---\n",
    "epochs = 2  # Liczba epok (pełnych przejść przez dane)\n",
    "\n",
    "\n",
    "# Pętla treningowa\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start epoki {epoch + 1}')\n",
    "\n",
    "    # Pętla po każdej partii danych\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Przejście w przód (forward pass)\n",
    "            loss_value = loss_fn(y_batch_train, logits)   # Obliczenie wartości straty\n",
    "\n",
    "        # Obliczenie gradientów i aktualizacja wag\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logowanie wartości straty co 200 kroków\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoka {epoch + 1}, krok {step}: strata = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
