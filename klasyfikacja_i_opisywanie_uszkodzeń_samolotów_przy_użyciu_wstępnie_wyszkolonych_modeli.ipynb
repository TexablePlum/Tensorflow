{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ustawiono środowiskowe zmienne konfiguracyjne TensorFlow, aby wyłączyć OneDNN i ograniczyć logi\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Załadowano niezbędne biblioteki do przetwarzania danych, budowy modelu i wizualizacji wyników\n",
    "import zipfile\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Zdefiniowano niestandardową warstwę BLIP do generowania opisów i streszczeń na podstawie obrazów\n",
    "class BlipCaptionSummaryLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, processor, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Inicjalizacja niestandardowej warstwy Keras z wykorzystaniem BLIP processor oraz modelu.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, image_path, task):\n",
    "        # Wykorzystano tf.py_function do wykonania niestandardowego przetwarzania obrazu oraz generacji tekstu\n",
    "        return tf.py_function(self.process_image, [image_path, task], tf.string)\n",
    "\n",
    "    def process_image(self, image_path, task):\n",
    "        \"\"\"\n",
    "        Wczytano obraz, wykonano jego przetwarzanie oraz wygenerowano opis.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Zdekodowano ścieżkę obrazu z tensora TensorFlow na string\n",
    "            image_path_str = image_path.numpy().decode(\"utf-8\")\n",
    "            # Otworzono obraz z wykorzystaniem PIL i skonwertowano na format RGB\n",
    "            image = Image.open(image_path_str).convert(\"RGB\")\n",
    "\n",
    "            # W zależności od zadania przypisano odpowiedni prompt\n",
    "            if task.numpy().decode(\"utf-8\") == \"caption\":\n",
    "                prompt = \"This is a picture of\"\n",
    "            else:\n",
    "                prompt = \"This is a detailed photo showing\"\n",
    "\n",
    "            # Przygotowano wejście do modelu BLIP przy użyciu processor'a\n",
    "            inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Wygenerowano wyjście tekstowe przy użyciu modelu BLIP\n",
    "            output = self.model.generate(**inputs)\n",
    "            # Zdekodowano wynik do czytelnego stringa\n",
    "            result = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            # W przypadku błędu wyświetlono komunikat i zwrócono informację o błędzie\n",
    "            print(f\"Error: {e}\")\n",
    "            return \"Error processing image\"\n",
    "\n",
    "def generate_text(image_path, task):\n",
    "    \"\"\"\n",
    "    Funkcja generująca tekst (opis/streszczenie) dla obrazu.\n",
    "    Tworzona jest instancja niestandardowej warstwy BLIP i wykonywane są odpowiednie przekształcenia.\n",
    "    \"\"\"\n",
    "    blip_layer = BlipCaptionSummaryLayer(BlipProcessor, model)\n",
    "    return blip_layer(image_path, task)\n",
    "\n",
    "def plot_image_with_title(image, model, true_label, predicted_label, class_names):\n",
    "    \"\"\"\n",
    "    Wyświetlono pojedynczy obraz wraz z prawidłową etykietą oraz predykcją modelu.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    # Przypisano nazwy klas na podstawie indeksów\n",
    "    true_label_name = class_names[true_label]\n",
    "    pred_label_name = class_names[predicted_label]\n",
    "    plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def test_model_on_image(test_generator, model, index_to_plot=0):\n",
    "    \"\"\"\n",
    "    Przeprowadzono test modelu na wybranym obrazie z zestawu testowego.\n",
    "    Pobierana jest partia obrazów, wyliczane są predykcje i wyświetlany jest obraz wraz z etykietami.\n",
    "    \"\"\"\n",
    "    test_images, test_labels = next(test_generator)\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "    class_indices = test_generator.class_indices\n",
    "    class_names = {v: k for k, v in class_indices.items()}\n",
    "    image_to_plot = test_images[index_to_plot]\n",
    "    true_label = test_labels[index_to_plot]\n",
    "    predicted_label = predicted_classes[index_to_plot]\n",
    "    plot_image_with_title(image=image_to_plot, model=model, true_label=true_label, predicted_label=predicted_label, class_names=class_names)\n",
    "\n",
    "# Ustalono ziarno dla powtarzalności wyników\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Ustalono parametry treningu oraz rozmiar przetwarzanych obrazów\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "# Pobrano zbiór danych z archiwum tar\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZjXM4RKxlBK9__ZjHBLl5A/aircraft-damage-dataset-v1.tar\"\n",
    "tar_filename = \"aircraft_damage_dataset_v1.tar\"\n",
    "extracted_folder = \"aircraft_damage_dataset_v1\"\n",
    "\n",
    "urllib.request.urlretrieve(url, tar_filename)\n",
    "print(f\"Pobrano {tar_filename}. Rozpoczęto ekstrakcję zawartości.\")\n",
    "\n",
    "if os.path.exists(extracted_folder):\n",
    "    print(f\"Folder '{extracted_folder}' już istnieje. Usunięto istniejący folder w celu uniknięcia duplikacji.\")\n",
    "    shutil.rmtree(extracted_folder)\n",
    "    print(f\"Usunięto folder: {extracted_folder}\")\n",
    "\n",
    "with tarfile.open(tar_filename, \"r\") as tar_ref:\n",
    "    tar_ref.extractall()\n",
    "    print(f\"Rozpakowano {tar_filename} pomyślnie.\")\n",
    "\n",
    "# Zdefiniowano katalogi dla podziału na zbiory treningowy, walidacyjny oraz testowy\n",
    "extract_path = \"aircraft_damage_dataset_v1\"\n",
    "train_dir = os.path.join(extract_path, 'train')\n",
    "test_dir = os.path.join(extract_path, 'test')\n",
    "valid_dir = os.path.join(extract_path, 'valid')\n",
    "\n",
    "# Utworzono ImageDataGenerator'y do przetwarzania danych obrazowych\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    seed=seed_value,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=valid_dir,\n",
    "    class_mode='binary',\n",
    "    seed=seed_value,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    target_size=(img_rows, img_cols)\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    class_mode='binary',\n",
    "    seed=seed_value,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    target_size=(img_rows, img_cols)\n",
    ")\n",
    "\n",
    "# Wczytano bazowy model VGG16 z wagami z ImageNet bez warstwy w pełni połączonej\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, 3))\n",
    "output = base_model.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "base_model = Model(base_model.input, output)\n",
    "\n",
    "# Zamrożono warstwy modelu VGG16\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Zbudowano model własny, dodając kolejne warstwy gęste, Dropout oraz warstwę wyjściową\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Przeprowadzono trening modelu przy użyciu generatora danych treningowych oraz walidacyjnych\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=valid_generator\n",
    ")\n",
    "\n",
    "train_history = model.history.history\n",
    "\n",
    "# Wykres przedstawiający stratę treningową\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(train_history['loss'])\n",
    "plt.show()\n",
    "\n",
    "# Wykres przedstawiający stratę walidacyjną\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(train_history['val_loss'])\n",
    "plt.show()\n",
    "\n",
    "# Wykres dokładności treningowej i walidacyjnej\n",
    "plt.plot(train_history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(train_history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.ylabel(\"Epochs\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Ewaluacja modelu na zbiorze testowym\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Testowanie modelu na wybranym obrazie ze zbioru testowego\n",
    "test_model_on_image(test_generator, model, index_to_plot=1)\n",
    "\n",
    "# Wczytano przetrenowany processor oraz model BLIP z repozytorium Salesforce\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Ścieżka do przykładowego obrazu z zestawu testowego\n",
    "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/144_10_JPG_jpg.rf.4d008cc33e217c1606b76585469d626b.jpg\")\n",
    "\n",
    "# Wygenerowano opis dla obrazu i wyświetlono wynik\n",
    "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
    "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# Wygenerowano streszczenie dla obrazu i wyświetlono wynik\n",
    "summary = generate_text(image_path, tf.constant(\"summary\"))\n",
    "print(\"Summary:\", summary.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# Wczytano kolejny obraz w celu wyświetlenia opisu i streszczenia\n",
    "image_url = \"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\"\n",
    "img = plt.imread(image_url)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\")\n",
    "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
    "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))\n",
    "summary = generate_text(image_path, tf.constant(\"summary\"))\n",
    "print(\"Summary:\", summary.numpy().decode(\"utf-8\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
