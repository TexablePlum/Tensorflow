{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odcinek: 1/50, Wynik: 55, Epsilon: 1.00\n",
      "Odcinek: 2/50, Wynik: 29, Epsilon: 0.99\n",
      "Odcinek: 3/50, Wynik: 30, Epsilon: 0.99\n",
      "Odcinek: 4/50, Wynik: 40, Epsilon: 0.99\n",
      "Odcinek: 5/50, Wynik: 11, Epsilon: 0.98\n",
      "Odcinek: 6/50, Wynik: 22, Epsilon: 0.98\n",
      "Odcinek: 7/50, Wynik: 46, Epsilon: 0.97\n",
      "Odcinek: 8/50, Wynik: 12, Epsilon: 0.97\n",
      "Odcinek: 9/50, Wynik: 34, Epsilon: 0.96\n",
      "Odcinek: 10/50, Wynik: 37, Epsilon: 0.96\n",
      "Odcinek: 11/50, Wynik: 9, Epsilon: 0.95\n",
      "Odcinek: 12/50, Wynik: 37, Epsilon: 0.95\n",
      "Odcinek: 13/50, Wynik: 17, Epsilon: 0.94\n",
      "Odcinek: 14/50, Wynik: 27, Epsilon: 0.94\n",
      "Odcinek: 15/50, Wynik: 12, Epsilon: 0.93\n",
      "Odcinek: 16/50, Wynik: 8, Epsilon: 0.93\n",
      "Odcinek: 17/50, Wynik: 11, Epsilon: 0.92\n",
      "Odcinek: 18/50, Wynik: 11, Epsilon: 0.92\n",
      "Odcinek: 19/50, Wynik: 10, Epsilon: 0.91\n",
      "Odcinek: 20/50, Wynik: 18, Epsilon: 0.91\n",
      "Odcinek: 21/50, Wynik: 28, Epsilon: 0.90\n",
      "Odcinek: 22/50, Wynik: 41, Epsilon: 0.90\n",
      "Odcinek: 23/50, Wynik: 9, Epsilon: 0.90\n",
      "Odcinek: 24/50, Wynik: 13, Epsilon: 0.89\n",
      "Odcinek: 25/50, Wynik: 19, Epsilon: 0.89\n",
      "Odcinek: 26/50, Wynik: 30, Epsilon: 0.88\n",
      "Odcinek: 27/50, Wynik: 33, Epsilon: 0.88\n",
      "Odcinek: 28/50, Wynik: 10, Epsilon: 0.87\n",
      "Odcinek: 29/50, Wynik: 19, Epsilon: 0.87\n",
      "Odcinek: 30/50, Wynik: 19, Epsilon: 0.86\n",
      "Odcinek: 31/50, Wynik: 10, Epsilon: 0.86\n",
      "Odcinek: 32/50, Wynik: 15, Epsilon: 0.86\n",
      "Odcinek: 33/50, Wynik: 10, Epsilon: 0.85\n",
      "Odcinek: 34/50, Wynik: 12, Epsilon: 0.85\n",
      "Odcinek: 35/50, Wynik: 13, Epsilon: 0.84\n",
      "Odcinek: 36/50, Wynik: 9, Epsilon: 0.84\n",
      "Odcinek: 37/50, Wynik: 11, Epsilon: 0.83\n",
      "Odcinek: 38/50, Wynik: 22, Epsilon: 0.83\n",
      "Odcinek: 39/50, Wynik: 15, Epsilon: 0.83\n",
      "Odcinek: 40/50, Wynik: 13, Epsilon: 0.82\n",
      "Odcinek: 41/50, Wynik: 10, Epsilon: 0.82\n",
      "Odcinek: 42/50, Wynik: 17, Epsilon: 0.81\n",
      "Odcinek: 43/50, Wynik: 9, Epsilon: 0.81\n",
      "Odcinek: 44/50, Wynik: 10, Epsilon: 0.81\n",
      "Odcinek: 45/50, Wynik: 10, Epsilon: 0.80\n",
      "Odcinek: 46/50, Wynik: 8, Epsilon: 0.80\n",
      "Odcinek: 47/50, Wynik: 16, Epsilon: 0.79\n",
      "Odcinek: 48/50, Wynik: 23, Epsilon: 0.79\n",
      "Odcinek: 49/50, Wynik: 10, Epsilon: 0.79\n",
      "Odcinek: 50/50, Wynik: 9, Epsilon: 0.78\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "\n",
    "# Wyłączenie ostrzeżeń (dla czytelności konsoli)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ustawienie losowości, by uzyskać powtarzalne wyniki\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Tworzymy środowisko CartPole\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Rozmiar wejścia (stan gry) oraz liczba możliwych akcji (lewo/prawo)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Parametry uczenia\n",
    "episodes = 50             # Liczba odcinków do trenowania\n",
    "batch_size = 32           # Wielkość paczki doświadczeń do uczenia\n",
    "gamma = 0.95              # Współczynnik dyskontowania przyszłych nagród\n",
    "epsilon = 1.0             # Początkowy współczynnik eksploracji (losowe decyzje)\n",
    "epsilon_min = 0.01        # Minimalny współczynnik eksploracji\n",
    "epsilon_decay = 0.995     # Tempo zmniejszania eksploracji\n",
    "\n",
    "# Tworzymy pamięć do przechowywania doświadczeń\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "# Funkcja budująca model sieci neuronowej\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(state_size,)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "model = build_model(state_size, action_size)\n",
    "\n",
    "# Zapamiętywanie doświadczeń\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Wybór akcji na podstawie strategii epsilon-greedy\n",
    "def act(state):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    q_values = model.predict(state, verbose=0)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "# Trenowanie modelu na podstawie doświadczeń (Replay memory)\n",
    "def replay(batch_size):\n",
    "    global epsilon\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "\n",
    "    states = np.vstack([exp[0] for exp in minibatch])\n",
    "    actions = [exp[1] for exp in minibatch]\n",
    "    rewards = [exp[2] for exp in minibatch]\n",
    "    next_states = np.vstack([exp[3] for exp in minibatch])\n",
    "    dones = [exp[4] for exp in minibatch]\n",
    "\n",
    "    q_next = model.predict(next_states, verbose=0)\n",
    "    q_target = model.predict(states, verbose=0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        target = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target += gamma * np.amax(q_next[i])\n",
    "        q_target[i][actions[i]] = target\n",
    "\n",
    "    model.fit(states, q_target, epochs=1, verbose=0)\n",
    "\n",
    "    # Zmniejszenie epsilon (coraz mniej losowych ruchów)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# Główna pętla treningowa\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset(seed=42)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for time in range(200):  # maksymalna liczba kroków w epizodzie\n",
    "        action = act(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Dostosowanie kształtu stanu\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        # Zapamiętanie doświadczenia\n",
    "        remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"Odcinek: {e+1}/{episodes}, Wynik: {time}, Epsilon: {epsilon:.2f}\")\n",
    "            break\n",
    "\n",
    "    # Trenowanie modelu po każdym odcinku, jeśli pamięć jest odpowiednio duża\n",
    "    if len(memory) >= batch_size:\n",
    "        replay(batch_size)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
