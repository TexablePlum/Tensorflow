{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niestandardowy Callback dla Zaawansowanego Logowania:\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Krok 1: Konfiguracja środowiska\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacja wartości pikseli do zakresu od 0 do 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Utworzenie zbioru danych z partiami do trenowania\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# Krok 2: Definicja modelu\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Spłaszcz wejście do wektora 1D\n",
    "    Dense(128, activation='relu'),  # Pierwsza warstwa ukryta z 128 neuronami i aktywacją ReLU\n",
    "    Dense(10)  # Warstwa wyjściowa z 10 neuronami dla 10 klas (cyfry 0-9)\n",
    "])\n",
    "\n",
    "# Krok 3: Definicja funkcji straty, optymalizatora i metryki\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Funkcja straty dla klasyfikacji wieloklasowej\n",
    "optimizer = tf.keras.optimizers.Adam()  # Optymalizator Adam dla efektywnego trenowania\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metryka do monitorowania dokładności podczas trenowania\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Krok 4: Implementacja niestandardowego callbacku \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'Koniec epoki {epoch + 1}, strata: {logs.get(\"loss\")}, dokładność: {logs.get(\"accuracy\")}')\n",
    "\n",
    "# Krok 5: Implementacja pętli trenowania z niestandardowym callbackiem\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Inicjalizacja niestandardowego callbacku\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Początek epoki {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Propagacja w przód: Obliczenie predykcji\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Obliczenie straty\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Obliczenie gradientów\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Zastosowanie gradientów do aktualizacji wag modelu\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Aktualizacja metryki dokładności\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Logowanie straty i dokładności co 200 kroków\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoka {epoch + 1} Krok {step}: Strata = {loss_value.numpy()} Dokładność = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Wywołanie niestandardowego callbacku na końcu każdej epoki\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Resetowanie metryki na końcu każdej epoki\n",
    "    accuracy_metric.reset_state()  # Użyj reset_state() zamiast reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodawanie metryki dokładności\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "\n",
    "# Krok 1: Konfiguracja środowiska\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacja wartości pikseli do zakresu od 0 do 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "\n",
    "# Utworzenie zbioru danych z partiami do trenowania\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# Krok 2: Definicja modelu\n",
    "\n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(28, 28)),  # Spłaszczenie wejścia do wektora 1D\n",
    "    Dense(128, activation='relu'),  # Pierwsza warstwa ukryta z 128 neuronami i aktywacją ReLU\n",
    "    Dense(10)  # Warstwa wyjściowa z 10 neuronami dla 10 klas (cyfry 0-9)\n",
    "])\n",
    "\n",
    "# Krok 3: Definicja funkcji straty, optymalizatora i metryki\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Funkcja straty dla klasyfikacji wieloklasowej\n",
    "optimizer = tf.keras.optimizers.Adam()  # Optymalizator Adam dla efektywnego trenowania\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metryka do śledzenia dokładności podczas trenowania\n",
    "\n",
    "# Krok 4: Implementacja pętli trenowania z metryką dokładności\n",
    "\n",
    "epochs = 5  # Liczba epok trenowania\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Początek epoki {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Propagacja w przód: Obliczenie predykcji\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Obliczenie straty\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Obliczenie gradientów\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Aktualizacja wag modelu poprzez zastosowanie gradientów\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Aktualizacja metryki dokładności\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Logowanie straty i dokładności co 200 kroków\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoka {epoch + 1} Krok {step}: Strata = {loss_value.numpy()} Dokładność = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Resetowanie metryki na końcu każdej epoki\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 10:52:28.166021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742896348.329389    2594 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742896348.368198    2594 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742896348.667158    2594 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742896348.667202    2594 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742896348.667206    2594 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742896348.667210    2594 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-25 10:52:28.700482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1742896356.741059    2594 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.432652711868286\n",
      "Epoch 1 Step 200: Loss = 0.36350858211517334\n",
      "Epoch 1 Step 400: Loss = 0.18423596024513245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Compute gradients and update weights\u001b[39;00m\n\u001b[32m     50\u001b[39m grads = tape.gradient(loss_value, model.trainable_weights)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Logging the loss every 200 steps\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[32m200\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:383\u001b[39m, in \u001b[36mBaseOptimizer.apply_gradients\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[32m    382\u001b[39m     grads, trainable_variables = \u001b[38;5;28mzip\u001b[39m(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterations\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:448\u001b[39m, in \u001b[36mBaseOptimizer.apply\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    445\u001b[39m     grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:511\u001b[39m, in \u001b[36mBaseOptimizer._backend_apply_gradients\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    508\u001b[39m     \u001b[38;5;28mself\u001b[39m._apply_weight_decay(trainable_variables)\n\u001b[32m    510\u001b[39m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_ema:\n\u001b[32m    516\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_model_variables_moving_average(\n\u001b[32m    517\u001b[39m         \u001b[38;5;28mself\u001b[39m._trainable_variables\n\u001b[32m    518\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:120\u001b[39m, in \u001b[36mTFOptimizer._backend_update_step\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    118\u001b[39m grads_and_vars = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[32m    119\u001b[39m grads_and_vars = \u001b[38;5;28mself\u001b[39m._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__internal__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistribute\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[39m, in \u001b[36mmaybe_merge_call\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[33;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \u001b[33;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m       fn, args=args, kwargs=kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:134\u001b[39m, in \u001b[36mTFOptimizer._distributed_tf_update_step\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.update_step(grad, var, learning_rate)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[43mdistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[39m, in \u001b[36mStrategyExtendedV2.update\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3002\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3004\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._replica_ctx_update(\n\u001b[32m   3008\u001b[39m       var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[39m, in \u001b[36m_DefaultDistributionExtended._update\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[39m, in \u001b[36m_DefaultDistributionExtended._update_non_slot\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[39m, in \u001b[36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    595\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[39m, in \u001b[36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/optimizers/adam.py:119\u001b[39m, in \u001b[36mAdam.update_step\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    117\u001b[39m lr = ops.cast(learning_rate, variable.dtype)\n\u001b[32m    118\u001b[39m gradient = ops.cast(gradient, variable.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m local_step = ops.cast(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m, variable.dtype)\n\u001b[32m    120\u001b[39m beta_1_power = ops.power(\n\u001b[32m    121\u001b[39m     ops.cast(\u001b[38;5;28mself\u001b[39m.beta_1, variable.dtype), local_step\n\u001b[32m    122\u001b[39m )\n\u001b[32m    123\u001b[39m beta_2_power = ops.power(\n\u001b[32m    124\u001b[39m     ops.cast(\u001b[38;5;28mself\u001b[39m.beta_2, variable.dtype), local_step\n\u001b[32m    125\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/backend/common/variables.py:451\u001b[39m, in \u001b[36mVariable.__add__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/sparse.py:493\u001b[39m, in \u001b[36melementwise_binary_union.<locals>.wrap_elementwise_binary_union.<locals>.sparse_wrapper\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x2, tf.IndexedSlices):\n\u001b[32m    491\u001b[39m     \u001b[38;5;66;03m# x2 is an IndexedSlices, densify.\u001b[39;00m\n\u001b[32m    492\u001b[39m     x2 = tf.convert_to_tensor(x2)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py:106\u001b[39m, in \u001b[36madd\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    101\u001b[39m dtype = dtypes.result_type(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(x1, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(x1)),\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(x2, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(x2)),\n\u001b[32m    104\u001b[39m )\n\u001b[32m    105\u001b[39m x1 = convert_to_tensor(x1, dtype)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m x2 = \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Special case of `tf.add`: `tf.nn.bias_add`\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# `BiasAdd` can be fused with `MatMul` and `Conv*` kernels\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Expecting `x1` to be `inputs` and `x2` to be `bias` (no swapping)\u001b[39;00m\n\u001b[32m    111\u001b[39m x2_squeeze_shape = [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m x2.shape.as_list() \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m d > \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:138\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(x, dtype, sparse, ragged)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf.is_tensor(x):\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype == \u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_int_dtype(dtype):\n\u001b[32m    135\u001b[39m         \u001b[38;5;66;03m# TensorFlow conversion is stricter than other backends, it does not\u001b[39;00m\n\u001b[32m    136\u001b[39m         \u001b[38;5;66;03m# allow ints for bools or floats for ints. We convert without dtype\u001b[39;00m\n\u001b[32m    137\u001b[39m         \u001b[38;5;66;03m# and cast instead.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         x = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.cast(x, dtype)\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.convert_to_tensor(x, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion.py:161\u001b[39m, in \u001b[36mconvert_to_tensor_v2_with_dispatch\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m.tf_export(\u001b[33m\"\u001b[39m\u001b[33mconvert_to_tensor\u001b[39m\u001b[33m\"\u001b[39m, v1=[])\n\u001b[32m     97\u001b[39m \u001b[38;5;129m@dispatch\u001b[39m.add_dispatch_support\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[32m     99\u001b[39m     value, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    100\u001b[39m ) -> tensor_lib.Tensor:\n\u001b[32m    101\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[32m    102\u001b[39m \n\u001b[32m    103\u001b[39m \u001b[33;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion.py:171\u001b[39m, in \u001b[36mconvert_to_tensor_v2\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proj/tf/tf/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:164\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_error_prefix\u001b[39m(msg, *, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    161\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m msg \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(value,\n\u001b[32m    165\u001b[39m             dtype=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    166\u001b[39m             name=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    167\u001b[39m             as_ref=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    168\u001b[39m             preferred_dtype=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    169\u001b[39m             accepted_result_types=(core.Symbol,)):\n\u001b[32m    170\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts `value` to a `Tensor` using registered conversion functions.\u001b[39;00m\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m \u001b[33;03m      value.\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m    202\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Podstawowa niestandardowa pętla trenowania\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# Wyłączanie wszystkich ostrzeżeń Pythona\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ustawienie poziomu logowania TensorFlow, aby wyciszyć ostrzeżenia i komunikaty informacyjne\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Krok 1: Konfiguracja środowiska\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "\n",
    "# Krok 2: Definicja modelu\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "# Krok 3: Definicja funkcji straty i optymalizatora\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Krok 4: Implementacja niestandardowej pętli trenowania\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Początek epoki {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Propagacja w przód\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Obliczenie straty\n",
    "\n",
    "        # Obliczenie gradientów i aktualizacja wag\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logowanie straty co 200 kroków\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoka {epoch + 1} Krok {step}: Strata = {loss_value.numpy()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
